{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corpora\n",
    "\n",
    "This notebook serves to demonstrate corpus loading and preprocessing without the aid of NLTK's corpus reader. We will use [ECHOE](https://github.com/ECHOEProject/echoe) as an example, and for the purposes of this demo we will process its distributed plaintext corpus rather than work directly from XML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,glob\n",
    "# The following import requires you to install the `gitpython` package,\n",
    "# *and* needs you to have Git installed and configured.\n",
    "# Alternatively, download it into `echoe/` manually,\n",
    "# comment out the following line, and skip running the first cell of code below:\n",
    "from git import Repo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll determine whether we have cloned the ECHOE repository before (in which case we only want to update it to the latest state) or not (in which case we'll want to clone it from remote).\n",
    "\n",
    "Don't worry about this part unless you're already curious how to wield Git within Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "remote = 'https://github.com/ECHOEProject/echoe.git'\n",
    "local = 'echoe'\n",
    "# Only clone if the target folder doesn't already exist:\n",
    "if not(os.path.exists(local)):\n",
    "    repo = Repo.clone_from(remote, local)\n",
    "# Else, just update the working copy from remote:\n",
    "else:\n",
    "    repo = Repo(local)\n",
    "    assert isinstance(repo, Repo)\n",
    "    repo.remotes.origin.pull()\n",
    "assert not repo.bare\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ECHOE line divisions are meaningful (though not entirely objective): each line contains either a manuscript rubric or a syntactical unit (roughly, a sentence or clause), or it is empty to separate between two \"chunks\" of text or between a rubric and the text body. Depending on your needs, it may make sense to use the `.readlines()` object function to read in lines separately. But for today, let's say our needs are the following:\n",
    "\n",
    "- Each ECHOE document (i.e. homily or saint's life) should be its own data container;\n",
    "- Each data container should be a single list of tokens;\n",
    "- We should still be able to identify which container is which ECHOE document, so the overarching data container should be a dictionary with keys like `049B.41`;\n",
    "- We want to get rid of the ECHOE references found at the start of each line of body text (e.g. \"56.33.11:\");\n",
    "- We want a normalized, case folded text, but ECHOE's plaintext corpus as distributed is already in the desired state.\n",
    "\n",
    "In this case it still makes sense to use `.readlines()`, just so we can filter out the segment references. So let's get started:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "echoe = dict()\n",
    "# Compile a list of files to be read in,\n",
    "# and run the rest of the cell on a per-file basis:\n",
    "for file in sorted(glob.glob('echoe/plaintext/*.txt')):\n",
    "    # Keep the base file name as an identifier:\n",
    "    identifier = os.path.basename(file).replace('.txt', '')\n",
    "    # Read in the document one line at a time:\n",
    "    segments = open(file).readlines()\n",
    "    # Initialize an empty list of tokens:\n",
    "    tokens = []\n",
    "    # Run the next few lines on a per-line basis:\n",
    "    for segment in segments:\n",
    "        # If the line begins with an identifier, discard the identifier:\n",
    "        if ': ' in segment:\n",
    "            segment = segment.split(': ')[1]\n",
    "        # Tokenize the rest of the line and add the output to the list of tokens:\n",
    "        tokens.extend(segment.rstrip(' \\n').split())\n",
    "    # Finally, store the list of tokens in the dictionary, using the identifier as a key:\n",
    "    echoe[identifier] = tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having read in our tokens, we can now access them as follows (limiting to 20 for convenience):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['to',\n",
       " 'eallum',\n",
       " 'folce',\n",
       " 'leofan',\n",
       " 'men',\n",
       " 'ælcne',\n",
       " 'þara',\n",
       " 'ic',\n",
       " 'bidde',\n",
       " 'þe',\n",
       " 'godes',\n",
       " 'ege',\n",
       " 'hæbbe',\n",
       " 'þæt',\n",
       " 'he',\n",
       " 'understande',\n",
       " 'his',\n",
       " 'agene',\n",
       " 'þearfe',\n",
       " 'gelæste']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "echoe['049B.11'][:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, ECHOE's distributed plaintext corpus is already fully normalized and case folded. When working with a corpus that is not, you can define a normalization function. The most efficient point to do this is immediately after reading the file or its lines (i.e. after `.readlines()` above), because then you have the smallest number of strings to process. But now that we're here, we can normalize on a per-token basis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dict with characters we want replaced:\n",
    "substitutions = {\n",
    "    'ę': 'æ',\n",
    "    'ƿ': 'w',\n",
    "    'ẏ': 'y',\n",
    "    'v': 'u',\n",
    "    'j': 'i'\n",
    "}\n",
    "\n",
    "# Write a function carrying out the desired operations:\n",
    "def normalize(token):\n",
    "    # Lowercase:\n",
    "    token = token.lower()\n",
    "    for k,v in substitutions.items():\n",
    "        # Carry out replacements:\n",
    "        token = token.lower().replace(k, v)\n",
    "    return token\n",
    "        \n",
    "# Let's use list comprehension to create a new list for each document:\n",
    "echoe_normalized = dict()\n",
    "for k,v in echoe.items():\n",
    "    new_doc = [normalize(token) for token in v]\n",
    "    echoe_normalized[k] = new_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['to',\n",
       " 'eallum',\n",
       " 'folce',\n",
       " 'leofan',\n",
       " 'men',\n",
       " 'ælcne',\n",
       " 'þara',\n",
       " 'ic',\n",
       " 'bidde',\n",
       " 'þe',\n",
       " 'godes',\n",
       " 'ege',\n",
       " 'hæbbe',\n",
       " 'þæt',\n",
       " 'he',\n",
       " 'understande',\n",
       " 'his',\n",
       " 'agene',\n",
       " 'þearfe',\n",
       " 'gelæste']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "echoe_normalized['049B.11'][:20]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our case it looks the same. If you want to test that it works, replace `.lower()` above with `.upper()`, or substitute out common vowels in the substitutions dictionary.\n",
    "\n",
    "Should we now want to run any routines on the corpus as a whole, without distinguishing between the documents, we can collect all our tokens into a single list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ECHOE contains 546959 total tokens.\n"
     ]
    }
   ],
   "source": [
    "all_tokens = []\n",
    "for doc in echoe.keys():\n",
    "    all_tokens.extend(echoe[doc])\n",
    "\n",
    "print(f\"ECHOE contains {len(all_tokens)} total tokens.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But some of the most interesting approaches now available to us involve the comparison of documents: for length, lexical diversity, etc. To this end, you may want to read up on [Pandas](https://realpython.com/pandas-dataframe/) first, to learn about the functional equivalent of spreadsheets, and [Matplotlib](https://realpython.com/python-matplotlib-guide/) next, to learn how to visualize the patterns you find."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
